{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from mafat_radar_challenge.main import get_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mafat_radar_challenge.data_loader.augmentation as module_aug\n",
    "import mafat_radar_challenge.data_loader.data_loaders as module_data\n",
    "import mafat_radar_challenge.model.loss as module_loss\n",
    "import mafat_radar_challenge.model.metric as module_metric\n",
    "import mafat_radar_challenge.model.model as module_arch\n",
    "import mafat_radar_challenge.data_loader.data_splitter as module_splitter\n",
    "from mafat_radar_challenge.cli import load_config\n",
    "from mafat_radar_challenge.main import get_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"/mnt/agarcia_HDD/mafat-radar-challenge-experiment/MAFAT_replica_aug_eff_b2_more_aux_more_synth_specaug_simple_aug_v5_adam/0808-140236/checkpoints/model_best.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch-visualizations packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_visualizations_master.src.guided_backprop import GuidedBackprop\n",
    "from pytorch_visualizations_master.src.vanilla_backprop import VanillaBackprop\n",
    "from pytorch_visualizations_master.src.misc_functions import save_gradient_images, convert_to_grayscale, get_positive_negative_saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_config(os.path.join(os.path.dirname(MODEL), \"config.yml\"))\n",
    "transforms = get_instance(module_aug, \"augmentation\", cfg)\n",
    "if \"sampler\" in cfg:\n",
    "    sampler = getattr(module_sampler, cfg[\"sampler\"][\"type\"])\n",
    "    sampler = partial(sampler, **cfg[\"sampler\"][\"args\"])\n",
    "else:\n",
    "    sampler = None\n",
    "# cfg[\"data_loader\"][\"args\"][\"sampler\"] = sampler\n",
    "data_loader = get_instance(module_data, \"data_loader\", cfg, transforms)\n",
    "valid_data_loader = get_instance(module_data, \"val_data_loader\", cfg, transforms)\n",
    "model = get_instance(module_arch, \"arch\", cfg)\n",
    "checkpoint = torch.load(MODEL)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model = model.model\n",
    "features = nn.ModuleList([])\n",
    "features.append(input_model._conv_stem)\n",
    "features.append(input_model._bn0)\n",
    "features.append(input_model._swish)\n",
    "features.append(input_model._blocks)\n",
    "features.append(input_model._conv_head)\n",
    "features.append(input_model._bn1)\n",
    "features.append(input_model._swish)\n",
    "\n",
    "input_model.features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_plotting(tensor):\n",
    "    \"\"\"Formats the shape of tensor for plotting.\n",
    "    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
    "    which is not suitable for plotting as images. This function formats an\n",
    "    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n",
    "    data.\n",
    "    Args:\n",
    "        tensor (torch.Tensor, torch.float32): Image tensor\n",
    "    Shape:\n",
    "        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n",
    "        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n",
    "    Return:\n",
    "        torch.Tensor (torch.float32): Formatted image tensor (detached)\n",
    "    Note:\n",
    "        Symbols used to describe dimensions:\n",
    "            - N: number of images in a batch\n",
    "            - C: number of channels\n",
    "            - H: height of the image\n",
    "            - W: width of the image\n",
    "    \"\"\"\n",
    "\n",
    "    has_batch_dimension = len(tensor.shape) == 4\n",
    "    formatted = tensor.clone()\n",
    "\n",
    "    if has_batch_dimension:\n",
    "        formatted = tensor.squeeze(0)\n",
    "\n",
    "    if formatted.shape[0] == 1:\n",
    "        return formatted.squeeze(0).detach()\n",
    "    else:\n",
    "        return formatted.permute(1, 2, 0).detach()\n",
    "    \n",
    "def denormalize(tensor):\n",
    "    \"\"\"Reverses the normalisation on a tensor.\n",
    "    Performs a reverse operation on a tensor, so the pixel value range is\n",
    "    between 0 and 1. Useful for when plotting a tensor into an image.\n",
    "    Normalisation: (image - mean) / std\n",
    "    Denormalisation: image * std + mean\n",
    "    Args:\n",
    "        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n",
    "    Shape:\n",
    "        Input: :math:`(N, C, H, W)`\n",
    "        Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "    Return:\n",
    "        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n",
    "            values between [0, 1]\n",
    "    Note:\n",
    "        Symbols used to describe dimensions:\n",
    "            - N: number of images in a batch\n",
    "            - C: number of channels\n",
    "            - H: height of the image\n",
    "            - W: width of the image\n",
    "    \"\"\"\n",
    "\n",
    "    means = [0.485, 0.456, 0.406]\n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "    denormalized = tensor.clone()\n",
    "\n",
    "    for channel, mean, std in zip(denormalized[0], means, stds):\n",
    "        channel.mul_(std).add_(mean)\n",
    "\n",
    "    return denormalized\n",
    "\n",
    "def gradient_to_image(gradient):\n",
    "    gradient = gradient - gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    gradient = np.uint8(gradient * 255).transpose(1, 2, 0)\n",
    "    # Convert RBG to GBR\n",
    "    gradient = gradient[..., ::-1]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, (image_batch, label_batch) in enumerate(valid_data_loader):\n",
    "    print(\"Batch {}\".format(idx))\n",
    "    for idx_2, (image, label) in enumerate(zip(image_batch, label_batch)):\n",
    "        print(\"Image {}\".format(idx_2))\n",
    "        print(\"Label {}\".format(label))\n",
    "        image = image.unsqueeze(0)\n",
    "        image.requires_grad = True\n",
    "        label = label[None]\n",
    "        label = int(label[0][0].cpu().numpy())\n",
    "        # Guided backprop\n",
    "        GBP = GuidedBackprop(input_model, image, label)\n",
    "        # Get gradients\n",
    "        guided_grads = GBP.generate_gradients()\n",
    "        # Save colored gradients\n",
    "        gradient_image = gradient_to_image(guided_grads)\n",
    "        # image = denormalize(image)\n",
    "        image = image.detach().cpu().numpy()\n",
    "        image = image[0].transpose(1,2,0)\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.subplot(121)    \n",
    "        plt.imshow(gradient_image)\n",
    "        plt.subplot(122)   \n",
    "        plt.imshow(image)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
